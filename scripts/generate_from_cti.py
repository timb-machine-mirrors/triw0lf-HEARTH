import os
import time
import re
from pathlib import Path
from openai import OpenAI
from dotenv import load_dotenv
from PyPDF2 import PdfReader
from datetime import datetime
import requests
from bs4 import BeautifulSoup
import io

# Import duplicate detection functionality
try:
    from duplicate_detection import check_duplicates_for_new_submission
    DUPLICATE_DETECTION_AVAILABLE = True
except ImportError:
    print("‚ö†Ô∏è Duplicate detection module not available. Skipping duplicate checks.")
    DUPLICATE_DETECTION_AVAILABLE = False

load_dotenv()
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

CTI_INPUT_DIR = Path(".hearth/intel-drops/")
OUTPUT_DIR    = Path("Flames/")
PROCESSED_DIR = Path(".hearth/processed-intel-drops/")
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
PROCESSED_DIR.mkdir(parents=True, exist_ok=True)

def get_next_hunt_id():
    """Scans the Flames/ directory to find the next available hunt ID."""
    flames_dir = Path("Flames/")
    flames_dir.mkdir(exist_ok=True)
    max_id = 0
    hunt_pattern = re.compile(r"H-\d{4}-(\d{3,})\.md")
    for f in flames_dir.glob("H-*.md"):
        match = hunt_pattern.match(f.name)
        if match:
            current_id = int(match.group(1))
            if current_id > max_id:
                max_id = current_id
    return max_id + 1

SYSTEM_PROMPT = """You are a threat hunter generating HEARTH markdown files.
Each file MUST focus on exactly ONE MITRE ATT&CK technique - no exceptions.
You MUST scope the hypothesis to be as narrow and specific as possible.
CRITICAL: Do NOT list or enumerate techniques at the start of the markdown file.
The markdown MUST start directly with the hypothesis text, not a heading.

When selecting which technique to focus on from multiple TTPs, prioritize based on:
1. Actionability - Choose the TTP that is most observable in logs/telemetry
2. Impact - Prioritize TTPs that directly lead to adversary objectives
3. Uniqueness - Prefer TTPs that are more distinctive of the specific threat
4. Detection Gap - Consider TTPs that are commonly missed by security tools

All hypotheses must:
- Focus on a single, specific MITRE ATT&CK technique only
- Be behaviorally specific to that one TTP
- Include EXACTLY these four parts:
  1. Actor type (e.g., "Threat actors", "Adversaries", "Attackers")
  2. Precise behavior (specific commands, tools, or methods being used)
  3. Immediate tactical goal (what the technique directly achieves)
  4. Specific target (exact systems, services, or data affected)
- Use firm language (e.g., "Threat actors are‚Ä¶", "Adversaries are‚Ä¶")
- Avoid actor names in the hypothesis (they go in the Why section)
- Do not include MITRE technique numbers in the hypothesis
- The output MUST begin directly with the hypothesis text. DO NOT include a title or markdown heading.
- If it's a vulnerability, do not use the CVE name or state it's a known vulnerability but rather the technique name

Examples of BAD (too broad) hypotheses:
‚ùå "Threat actors are using PowerShell to execute malicious code"
‚ùå "Adversaries are exploiting vulnerabilities in web applications"
‚ùå "Attackers are using valid credentials to access systems"

Examples of GOOD (narrow) hypotheses:
‚úÖ "Threat actors are using PowerShell's Invoke-WebRequest cmdlet to download encrypted payloads from Discord CDN to evade network detection"
‚úÖ "Adversaries are modifying Windows Registry Run keys in HKCU\\Software\\Microsoft\\Windows\\CurrentVersion\\Run to maintain persistence across system reboots"
‚úÖ "Attackers are creating scheduled tasks with random 8-character alphanumeric names to execute Base64-encoded PowerShell commands at system startup"
"""

USER_TEMPLATE = """{regeneration_instruction}CTI REPORT:

{cti_text}

---

Instructions:
1.  Read the CTI Report.
2.  Select the single most actionable MITRE ATT&CK technique from the report.
3.  Write a specific, narrow, and actionable hunt hypothesis based on that technique.
4.  Write a "Why" section explaining the importance of the hunt.
5.  Write a "References" section with a link to the chosen MITRE technique and the source CTI.
6.  The output MUST be only the content for the hunt, starting with the hypothesis. DO NOT include a title or the metadata table.

Your output should look like this:

[Your extremely specific hypothesis]

| Hunt #       | Idea / Hypothesis                                                      | Tactic         | Notes                                                                              | Tags                           | Submitter           |
|--------------|-------------------------------------------------------------------------|----------------|------------------------------------------------------------------------------------|--------------------------------|---------------------|
| [Leave blank] | [Same hypothesis]                                                      | [MITRE Tactic] | Based on ATT&CK technique [Txxxx]. Generated by [hearth-auto-intel](https://github.com/THORCollective/HEARTH). | #[tactic] #[technique] #[tag] | {submitter_credit} |

## Why
- [Why this behavior is important to detect]
- [Tactical impact of success]
- [Links to larger campaigns]

## References
- [MITRE ATT&CK link]
- [Source CTI Report]({cti_source_url})
"""

def summarize_cti_with_map_reduce(text, model="gpt-4", max_tokens=128000):
    """
    Summarizes long text by splitting it into chunks, summarizing each, 
    and then creating a final summary of the summaries.
    This is a 'map-reduce' approach to handle large contexts.
    """
    # Estimate token count (very rough approximation)
    text_token_count = len(text) / 4  

    if text_token_count < max_tokens * 0.7:  # If text is well within the limit
        print("‚úÖ CTI content is within the context window. No summarization needed.")
        return text

    print(f"‚ö†Ô∏è CTI content is too long ({int(text_token_count)} tokens). Starting map-reduce summarization.")
    
    # 1. Map: Split the document into overlapping chunks
    chunk_size = int(max_tokens * 0.6) # Use 60% of the model's context for each chunk
    overlap = int(chunk_size * 0.1) # 10% overlap
    
    chunks = []
    start = 0
    while start < len(text):
        end = start + chunk_size
        chunks.append(text[start:end])
        start += chunk_size - overlap

    print(f"Split CTI into {len(chunks)} chunks.")

    # 2. Summarize each chunk
    chunk_summaries = []
    for i, chunk in enumerate(chunks):
        print(f"Summarizing chunk {i+1}/{len(chunks)}...")
        try:
            response = client.chat.completions.create(
                model=model,
                messages=[{"role":"user","content":
                    "This is one part of a larger threat intelligence report. "
                    "Extract the key actionable intelligence from this section. "
                    "Focus on specific tools, techniques, vulnerabilities, and adversary procedures. "
                    "Your output will be combined with others, so be concise and clear.\n\n"
                    f"--- CHUNK {i+1}/{len(chunks)} ---\n\n{chunk}"
                }],
                temperature=0.2,
            )
            summary = response.choices[0].message.content.strip()
            chunk_summaries.append(summary)
        except Exception as e:
            print(f"‚ùå Error summarizing chunk {i+1}: {e}")
            # If a chunk fails, we just add a note and continue
            chunk_summaries.append(f"[Could not summarize chunk {i+1}]")
            
    # 3. Reduce: Create a final summary from the individual summaries
    print("Creating final summary of all chunks...")
    combined_summary = "\n\n---\n\n".join(chunk_summaries)
    
    try:
        final_response = client.chat.completions.create(
            model=model,
            messages=[{"role":"user","content":
                "The following are summaries of different parts of a long threat intelligence report. "
                "Synthesize them into a single, coherent, and actionable report. "
                "Remove redundancy and create a clear narrative of the adversary's actions. "
                "The final output should be a comprehensive summary that can be used to generate a threat hunt.\n\n"
                f"--- COMBINED SUMMARIES ---\n\n{combined_summary}"
            }],
            temperature=0.2,
        )
        return final_response.choices[0].message.content.strip()
    except Exception as e:
        print(f"‚ùå Error creating final summary: {e}")
        # Fallback: return the combined summaries if the final step fails
        return f"WARNING: Final summarization failed. Combined summaries provided below:\n\n{combined_summary}"

def download_and_extract_text(url):
    """Downloads content from a URL and extracts text."""
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()

        if 'application/pdf' in response.headers.get('Content-Type', ''):
            pdf_reader = PdfReader(io.BytesIO(response.content))
            return "".join(page.extract_text() for page in pdf_reader.pages)
        else:
            soup = BeautifulSoup(response.content, 'html.parser')
            # Basic text extraction, can be improved
            for script in soup(["script", "style"]):
                script.extract()
            return " ".join(soup.stripped_strings)
    except requests.exceptions.RequestException as e:
        print(f"‚ùå Error downloading URL {url}: {e}")
        return None
    except Exception as e:
        print(f"‚ùå Error processing URL {url}: {e}")
        return None

def cleanup_hunt_body(ai_content):
    """
    Cleans the AI's raw output to ensure it starts with the hypothesis
    and removes any prepended conversational text or extra headers.
    """
    lines = ai_content.splitlines()
    
    # Find the first line that looks like a real hypothesis.
    # It should be a non-empty line that doesn't start with a known non-content keyword.
    first_content_index = -1
    for i, line in enumerate(lines):
        stripped_line = line.strip()
        if not stripped_line:
            continue
            
        # These are keywords we want to strip out if they appear before the hypothesis.
        is_unwanted_prefix = any(
            stripped_line.lower().startswith(prefix) for prefix in 
            ['cti report:', 'hypothesis:', '---', 'instructions:', 'your output should']
        )
        
        if not is_unwanted_prefix:
            first_content_index = i
            break
            
    if first_content_index == -1:
        print("‚ö†Ô∏è Could not find the start of the hypothesis. Returning raw content.")
        return ai_content

    # The hypothesis might have a "Hypothesis:" label. Let's remove that specifically.
    first_line = lines[first_content_index]
    if "hypothesis:" in first_line.lower():
        lines[first_content_index] = first_line.split(':', 1)[-1].strip()

    return "\n".join(lines[first_content_index:]).strip()

def generate_hunt_content(cti_text, cti_source_url, submitter_credit, is_regeneration=False):
    """Generate just the core content of a hunt from CTI text."""
    try:
        print("Starting CTI summarization...")
        summary = summarize_cti_with_map_reduce(cti_text)
        print("CTI summarization complete.")
        
        regeneration_instruction = ""
        temperature = 0.2
        if is_regeneration:
            print("üîÑ This is a regeneration. Requesting a new hypothesis.")
            regeneration_instruction = (
                "IMPORTANT: The previous attempt to generate a hunt from this CTI was not satisfactory. "
                "Your task is to generate a NEW and DIFFERENT hunt hypothesis. "
                "Analyze the CTI report again and focus on a different technique, a more specific behavior, "
                "or a unique, actionable aspect that was missed before. Do not repeat the previous hypothesis.\n\n"
            )
            temperature = 0.7

        prompt = USER_TEMPLATE.format(
            regeneration_instruction=regeneration_instruction,
            cti_text=summary, 
            cti_source_url=cti_source_url,
            submitter_credit=submitter_credit
        )
        response = client.chat.completions.create(
            model="gpt-4",
            messages=[{"role":"system","content":SYSTEM_PROMPT},
                     {"role":"user","content":prompt}],
            temperature=temperature,
            max_tokens=800
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        print(f"‚ùå Error generating hunt content: {str(e)}")
        return None

def read_file_content(file_path):
    """Read content from either PDF or text file."""
    if file_path.suffix.lower() == '.pdf':
        try:
            reader = PdfReader(file_path)
            text = ""
            for page in reader.pages:
                text += page.extract_text() + "\n"
            return text.strip()
        except Exception as e:
            print(f"Error reading PDF {file_path}: {str(e)}")
            return None
    else:
        try:
            return file_path.read_text()
        except Exception as e:
            print(f"Error reading file {file_path}: {str(e)}")
            return None

if __name__ == "__main__":
    existing_hunt_path = os.getenv("EXISTING_HUNT_FILE")
    is_regeneration = bool(existing_hunt_path)
    
    if existing_hunt_path:
        out_md_path = Path(existing_hunt_path)
        hunt_id = out_md_path.stem
        print(f"üîÑ Regenerating hunt for {hunt_id} at {out_md_path}")
    else:
        # Determine the next hunt number
        hunt_files = list(Path(".").glob("Flames/H*.md"))
        if hunt_files:
            # Filter files that have the expected format and extract numbers
            hunt_numbers = []
            for f in hunt_files:
                parts = f.stem.split('-')
                if len(parts) >= 3 and parts[-1].isdigit():
                    hunt_numbers.append(int(parts[-1]))
            
            if hunt_numbers:
                next_hunt_num = max(hunt_numbers) + 1
            else:
                next_hunt_num = 1
        else:
            next_hunt_num = 1
        
        year = datetime.now().year
        hunt_id = f"H-{year}-{next_hunt_num:03d}"
        out_md_path = Path(f"Flames/{hunt_id}.md")
        print(f"üå± Generating new hunt: {hunt_id}")

    cti_source_url = os.getenv("CTI_SOURCE_URL")
    if not cti_source_url:
        raise ValueError("‚ùå CTI_SOURCE_URL environment variable not set.")

    # Get CTI content
    cti_content = download_and_extract_text(cti_source_url)

    # Get submitter info
    submitter_name = os.getenv("SUBMITTER_NAME", "hearth-auto-intel")
    profile_link = os.getenv("PROFILE_LINK")
    if profile_link:
        submitter_credit = f"[{submitter_name}]({profile_link})"
    else:
        submitter_credit = submitter_name

    if cti_content:
        # 1. Generate the core hunt content from the AI
        hunt_body = generate_hunt_content(
            cti_content, 
            cti_source_url, 
            submitter_credit,
            is_regeneration=is_regeneration
        )

        if hunt_body:
            # 2. Clean up the AI's output
            cleaned_body = cleanup_hunt_body(hunt_body)

            # 3. Construct the final markdown content
            final_content = f"# {hunt_id}\n\n"
            final_content += cleaned_body.replace("| [Leave blank] |", f"| {hunt_id}    |")

            # 4. Save the hunt file
            with open(out_md_path, "w") as f:
                f.write(final_content)
            print(f"‚úÖ Successfully wrote hunt to {out_md_path}")
            
            # 5. Set the output for the GitHub Action
            if 'GITHUB_OUTPUT' in os.environ:
                with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
                    print(f'HUNT_FILE_PATH={out_md_path}', file=f)
                    print(f'HUNT_ID={hunt_id}', file=f)

            # 6. Run duplicate detection
            if DUPLICATE_DETECTION_AVAILABLE:
                print("üîç Running duplicate detection...")
                duplicate_analysis = check_duplicates_for_new_submission(final_content, out_md_path.name)
                print("Duplicate detection complete.")
                
                # Add duplicate detection result to GitHub output
                if 'GITHUB_OUTPUT' in os.environ:
                    with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
                        print(f'DUPLICATE_ANALYSIS<<EOF', file=f)
                        print(duplicate_analysis, file=f)
                        print(f'EOF', file=f)
            else:
                duplicate_analysis = "‚ö†Ô∏è Duplicate detection not available."
        else:
            print(f"Could not generate hunt content. Skipping file creation.")
    else:
        print(f"Could not retrieve CTI content. Skipping hunt generation.") 